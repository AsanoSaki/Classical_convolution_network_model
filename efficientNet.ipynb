{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### efficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import imagenet_utils\n",
    "from tensorflow.keras.applications.imagenet_utils import decode_predictions\n",
    "\n",
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 用于计算padding的大小\n",
    "def correct_pad(inputs, kernel_size):\n",
    "    img_dim = 1\n",
    "    input_size = backend.int_shape(inputs)[img_dim:(img_dim + 2)]\n",
    "\n",
    "    if isinstance(kernel_size, int):\n",
    "        kernel_size = (kernel_size, kernel_size)\n",
    "\n",
    "    if input_size[0] is None:\n",
    "        adjust = (1, 1)\n",
    "    else:\n",
    "        adjust = (1 - input_size[0] % 2, 1 - input_size[1] % 2)\n",
    "\n",
    "    correct = (kernel_size[0] // 2, kernel_size[1] // 2)\n",
    "\n",
    "    return ((correct[0] - adjust[0], correct[0]),\n",
    "            (correct[1] - adjust[1], correct[1]))\n",
    "\n",
    "\n",
    "# 用于下载模型的默认参数\n",
    "BASE_WEIGHTS_PATH = (\n",
    "    'https://github.com/Callidior/keras-applications/'\n",
    "    'releases/download/efficientnet/')\n",
    "WEIGHTS_HASHES = {\n",
    "    'b0': ('e9e877068bd0af75e0a36691e03c072c',\n",
    "           '345255ed8048c2f22c793070a9c1a130'),\n",
    "    'b1': ('8f83b9aecab222a9a2480219843049a1',\n",
    "           'b20160ab7b79b7a92897fcb33d52cc61'),\n",
    "    'b2': ('b6185fdcd190285d516936c09dceeaa4',\n",
    "           'c6e46333e8cddfa702f4d8b8b6340d70'),\n",
    "    'b3': ('b2db0f8aac7c553657abb2cb46dcbfbb',\n",
    "           'e0cf8654fad9d3625190e30d70d0c17d'),\n",
    "    'b4': ('ab314d28135fe552e2f9312b31da6926',\n",
    "           'b46702e4754d2022d62897e0618edc7b'),\n",
    "    'b5': ('8d60b903aff50b09c6acf8eaba098e09',\n",
    "           '0a839ac36e46552a881f2975aaab442f'),\n",
    "    'b6': ('a967457886eac4f5ab44139bdd827920',\n",
    "           '375a35c17ef70d46f9c664b03b4437f2'),\n",
    "    'b7': ('e964fd6e26e9a4c144bcb811f2a10f20',\n",
    "           'd55674cc46b805f4382d18bc08ed43c1')\n",
    "}\n",
    "\n",
    "# 每个Blocks的参数\n",
    "DEFAULT_BLOCKS_ARGS = [\n",
    "    {'kernel_size': 3, 'repeats': 1, 'filters_in': 32, 'filters_out': 16,\n",
    "     'expand_ratio': 1, 'id_skip': True, 'strides': 1, 'se_ratio': 0.25},\n",
    "    {'kernel_size': 3, 'repeats': 2, 'filters_in': 16, 'filters_out': 24,\n",
    "     'expand_ratio': 6, 'id_skip': True, 'strides': 2, 'se_ratio': 0.25},\n",
    "    {'kernel_size': 5, 'repeats': 2, 'filters_in': 24, 'filters_out': 40,\n",
    "     'expand_ratio': 6, 'id_skip': True, 'strides': 2, 'se_ratio': 0.25},\n",
    "    {'kernel_size': 3, 'repeats': 3, 'filters_in': 40, 'filters_out': 80,\n",
    "     'expand_ratio': 6, 'id_skip': True, 'strides': 2, 'se_ratio': 0.25},\n",
    "    {'kernel_size': 5, 'repeats': 3, 'filters_in': 80, 'filters_out': 112,\n",
    "     'expand_ratio': 6, 'id_skip': True, 'strides': 1, 'se_ratio': 0.25},\n",
    "    {'kernel_size': 5, 'repeats': 4, 'filters_in': 112, 'filters_out': 192,\n",
    "     'expand_ratio': 6, 'id_skip': True, 'strides': 2, 'se_ratio': 0.25},\n",
    "    {'kernel_size': 3, 'repeats': 1, 'filters_in': 192, 'filters_out': 320,\n",
    "     'expand_ratio': 6, 'id_skip': True, 'strides': 1, 'se_ratio': 0.25}\n",
    "]\n",
    "\n",
    "# 两个Kernel的初始化器\n",
    "CONV_KERNEL_INITIALIZER = {\n",
    "    'class_name': 'VarianceScaling',\n",
    "    'config': {\n",
    "        'scale': 2.0,\n",
    "        'mode': 'fan_out',\n",
    "        'distribution': 'normal'\n",
    "    }\n",
    "}\n",
    "\n",
    "DENSE_KERNEL_INITIALIZER = {\n",
    "    'class_name': 'VarianceScaling',\n",
    "    'config': {\n",
    "        'scale': 1. / 3.,\n",
    "        'mode': 'fan_out',\n",
    "        'distribution': 'uniform'\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def block(inputs, activation_fn=tf.nn.swish, drop_rate=0., name='',\n",
    "          filters_in=32, filters_out=16, kernel_size=3, strides=1,\n",
    "          expand_ratio=1, se_ratio=0., id_skip=True):\n",
    "\n",
    "    bn_axis = 3\n",
    "\n",
    "    # 升多少维度\n",
    "    filters = filters_in * expand_ratio\n",
    "\n",
    "    # 利用Inverted residuals\n",
    "    # part1 1x1升维度\n",
    "    if expand_ratio != 1:\n",
    "        x = layers.Conv2D(filters, 1,\n",
    "                          padding='same',\n",
    "                          use_bias=False,\n",
    "                          kernel_initializer=CONV_KERNEL_INITIALIZER,\n",
    "                          name=name + 'expand_conv')(inputs)\n",
    "        x = layers.BatchNormalization(axis=bn_axis, name=name + 'expand_bn')(x)\n",
    "        x = layers.Activation(activation_fn, name=name + 'expand_activation')(x)\n",
    "    else:\n",
    "        x = inputs\n",
    "\n",
    "    # padding\n",
    "    if strides == 2:\n",
    "        x = layers.ZeroPadding2D(padding=correct_pad(x, kernel_size),\n",
    "                                 name=name + 'dwconv_pad')(x)\n",
    "        conv_pad = 'valid'\n",
    "    else:\n",
    "        conv_pad = 'same'\n",
    "\n",
    "    # part2 利用3x3卷积对每一个channel进行卷积\n",
    "    x = layers.DepthwiseConv2D(kernel_size,\n",
    "                               strides=strides,\n",
    "                               padding=conv_pad,\n",
    "                               use_bias=False,\n",
    "                               depthwise_initializer=CONV_KERNEL_INITIALIZER,\n",
    "                               name=name + 'dwconv')(x)\n",
    "    x = layers.BatchNormalization(axis=bn_axis, name=name + 'bn')(x)\n",
    "    x = layers.Activation(activation_fn, name=name + 'activation')(x)\n",
    "\n",
    "    # 压缩后再放大,作为一个调整系数\n",
    "    if 0 < se_ratio <= 1:\n",
    "        filters_se = max(1, int(filters_in * se_ratio))\n",
    "        se = layers.GlobalAveragePooling2D(name=name + 'se_squeeze')(x)\n",
    "        se = layers.Reshape((1, 1, filters), name=name + 'se_reshape')(se)\n",
    "        se = layers.Conv2D(filters_se, 1,\n",
    "                           padding='same',\n",
    "                           activation=activation_fn,\n",
    "                           kernel_initializer=CONV_KERNEL_INITIALIZER,\n",
    "                           name=name + 'se_reduce')(se)\n",
    "        se = layers.Conv2D(filters, 1,\n",
    "                           padding='same',\n",
    "                           activation='sigmoid',\n",
    "                           kernel_initializer=CONV_KERNEL_INITIALIZER,\n",
    "                           name=name + 'se_expand')(se)\n",
    "        x = layers.multiply([x, se], name=name + 'se_excite')\n",
    "\n",
    "    # part3 利用1x1对特征层进行压缩\n",
    "    x = layers.Conv2D(filters_out, 1,\n",
    "                      padding='same',\n",
    "                      use_bias=False,\n",
    "                      kernel_initializer=CONV_KERNEL_INITIALIZER,\n",
    "                      name=name + 'project_conv')(x)\n",
    "    x = layers.BatchNormalization(axis=bn_axis, name=name + 'project_bn')(x)\n",
    "\n",
    "    # 实现残差神经网络\n",
    "    if (id_skip is True and strides == 1 and filters_in == filters_out):\n",
    "        if drop_rate > 0:\n",
    "            x = layers.Dropout(drop_rate,\n",
    "                               noise_shape=(None, 1, 1, 1),\n",
    "                               name=name + 'drop')(x)\n",
    "        x = layers.add([x, inputs], name=name + 'add')\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def EfficientNet(width_coefficient,\n",
    "                 depth_coefficient,\n",
    "                 default_size,\n",
    "                 dropout_rate=0.2,\n",
    "                 drop_connect_rate=0.2,\n",
    "                 depth_divisor=8,\n",
    "                 activation_fn=tf.nn.swish,\n",
    "                 blocks_args=DEFAULT_BLOCKS_ARGS,\n",
    "                 model_name='efficientnet',\n",
    "                 weights='imagenet',\n",
    "                 input_tensor=None,\n",
    "                 input_shape=None,\n",
    "                 pooling=None,\n",
    "                 classes=1000,\n",
    "                 **kwargs):\n",
    "\n",
    "    input_shape = [416,416,3]\n",
    "    img_input = layers.Input(tensor=input_tensor, shape=input_shape)\n",
    "\n",
    "\n",
    "    bn_axis = 3 \n",
    "\n",
    "    # 保证filter的大小可以被8整除\n",
    "    def round_filters(filters, divisor=depth_divisor):\n",
    "        \"\"\"Round number of filters based on depth multiplier.\"\"\"\n",
    "        filters *= width_coefficient\n",
    "        new_filters = max(divisor, int(filters + divisor / 2) // divisor * divisor)\n",
    "        # Make sure that round down does not go down by more than 10%.\n",
    "        if new_filters < 0.9 * filters:\n",
    "            new_filters += divisor\n",
    "        return int(new_filters)\n",
    "\n",
    "    # 重复次数，取顶\n",
    "    def round_repeats(repeats):\n",
    "        return int(math.ceil(depth_coefficient * repeats))\n",
    "\n",
    "    # Build stem\n",
    "    x = img_input\n",
    "    x = layers.ZeroPadding2D(padding=correct_pad(x, 3),\n",
    "                             name='stem_conv_pad')(x)\n",
    "    x = layers.Conv2D(round_filters(32), 3,\n",
    "                      strides=2,\n",
    "                      padding='valid',\n",
    "                      use_bias=False,\n",
    "                      kernel_initializer=CONV_KERNEL_INITIALIZER,\n",
    "                      name='stem_conv')(x)\n",
    "    x = layers.BatchNormalization(axis=bn_axis, name='stem_bn')(x)\n",
    "    x = layers.Activation(activation_fn, name='stem_activation')(x)\n",
    "\n",
    "    # Build blocks\n",
    "    from copy import deepcopy\n",
    "\n",
    "    # 防止参数的改变\n",
    "    blocks_args = deepcopy(blocks_args)\n",
    "\n",
    "    b = 0\n",
    "    # 计算总的block的数量\n",
    "    blocks = float(sum(args['repeats'] for args in blocks_args))\n",
    "    for (i, args) in enumerate(blocks_args):\n",
    "        assert args['repeats'] > 0\n",
    "        args['filters_in'] = round_filters(args['filters_in'])\n",
    "        args['filters_out'] = round_filters(args['filters_out'])\n",
    "\n",
    "        for j in range(round_repeats(args.pop('repeats'))):\n",
    "            if j > 0:\n",
    "                args['strides'] = 1\n",
    "                args['filters_in'] = args['filters_out']\n",
    "            x = block(x, activation_fn, drop_connect_rate * b / blocks,\n",
    "                      name='block{}{}_'.format(i + 1, chr(j + 97)), **args)\n",
    "            b += 1\n",
    "    \n",
    "    # 收尾工作\n",
    "    x = layers.Conv2D(round_filters(1280), 1,\n",
    "                      padding='same',\n",
    "                      use_bias=False,\n",
    "                      kernel_initializer=CONV_KERNEL_INITIALIZER,\n",
    "                      name='top_conv')(x)\n",
    "    x = layers.BatchNormalization(axis=bn_axis, name='top_bn')(x)\n",
    "    x = layers.Activation(activation_fn, name='top_activation')(x)\n",
    "\n",
    "    # 利用GlobalAveragePooling2D代替全连接层\n",
    "    x = layers.GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "    if dropout_rate > 0:\n",
    "        x = layers.Dropout(dropout_rate, name='top_dropout')(x)\n",
    "\n",
    "    x = layers.Dense(classes,\n",
    "                        activation='softmax',\n",
    "                        kernel_initializer=DENSE_KERNEL_INITIALIZER,\n",
    "                        name='probs')(x)\n",
    "\n",
    "    # 输入inputs\n",
    "    inputs = img_input\n",
    "\n",
    "    model = Model(inputs, x, name=model_name)\n",
    "\n",
    "    # Load weights.\n",
    "    if weights == 'imagenet':\n",
    "        file_suff = '_weights_tf_dim_ordering_tf_kernels_autoaugment.h5'\n",
    "        file_hash = WEIGHTS_HASHES[model_name[-2:]][0]\n",
    "        file_name = model_name + file_suff\n",
    "        weights_path = get_file(file_name,BASE_WEIGHTS_PATH + file_name,\n",
    "                                    cache_subdir='models',\n",
    "                                    file_hash=file_hash)\n",
    "        model.load_weights(weights_path)\n",
    "    elif weights is not None:\n",
    "        model.load_weights(weights)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EfficientNetB0(weights='imagenet',\n",
    "                   input_tensor=None,\n",
    "                   input_shape=None,\n",
    "                   pooling=None,\n",
    "                   classes=1000,\n",
    "                   **kwargs):\n",
    "    return EfficientNet(1.0, 1.0, 224, 0.2,\n",
    "                        model_name='efficientnet-b0',\n",
    "                        weights=weights,\n",
    "                        input_tensor=input_tensor, input_shape=input_shape,\n",
    "                        pooling=pooling, classes=classes,\n",
    "                        **kwargs)\n",
    "\n",
    "\n",
    "def EfficientNetB1(weights='imagenet',\n",
    "                   input_tensor=None,\n",
    "                   input_shape=None,\n",
    "                   pooling=None,\n",
    "                   classes=1000,\n",
    "                   **kwargs):\n",
    "    return EfficientNet(1.0, 1.1, 240, 0.2,\n",
    "                        model_name='efficientnet-b1',\n",
    "                        weights=weights,\n",
    "                        input_tensor=input_tensor, input_shape=input_shape,\n",
    "                        pooling=pooling, classes=classes,\n",
    "                        **kwargs)\n",
    "\n",
    "\n",
    "def EfficientNetB2(weights='imagenet',\n",
    "                   input_tensor=None,\n",
    "                   input_shape=None,\n",
    "                   pooling=None,\n",
    "                   classes=1000,\n",
    "                   **kwargs):\n",
    "    return EfficientNet(1.1, 1.2, 260, 0.3,\n",
    "                        model_name='efficientnet-b2',\n",
    "                        weights=weights,\n",
    "                        input_tensor=input_tensor, input_shape=input_shape,\n",
    "                        pooling=pooling, classes=classes,\n",
    "                        **kwargs)\n",
    "\n",
    "\n",
    "def EfficientNetB3(weights='imagenet',\n",
    "                   input_tensor=None,\n",
    "                   input_shape=None,\n",
    "                   pooling=None,\n",
    "                   classes=1000,\n",
    "                   **kwargs):\n",
    "    return EfficientNet(1.2, 1.4, 300, 0.3,\n",
    "                        model_name='efficientnet-b3',\n",
    "                        weights=weights,\n",
    "                        input_tensor=input_tensor, input_shape=input_shape,\n",
    "                        pooling=pooling, classes=classes,\n",
    "                        **kwargs)\n",
    "\n",
    "\n",
    "def EfficientNetB4(weights='imagenet',\n",
    "                   input_tensor=None,\n",
    "                   input_shape=None,\n",
    "                   pooling=None,\n",
    "                   classes=1000,\n",
    "                   **kwargs):\n",
    "    return EfficientNet(1.4, 1.8, 380, 0.4,\n",
    "                        model_name='efficientnet-b4',\n",
    "                        weights=weights,\n",
    "                        input_tensor=input_tensor, input_shape=input_shape,\n",
    "                        pooling=pooling, classes=classes,\n",
    "                        **kwargs)\n",
    "\n",
    "\n",
    "def EfficientNetB5(weights='imagenet',\n",
    "                   input_tensor=None,\n",
    "                   input_shape=None,\n",
    "                   pooling=None,\n",
    "                   classes=1000,\n",
    "                   **kwargs):\n",
    "    return EfficientNet(1.6, 2.2, 456, 0.4,\n",
    "                        model_name='efficientnet-b5',\n",
    "                        weights=weights,\n",
    "                        input_tensor=input_tensor, input_shape=input_shape,\n",
    "                        pooling=pooling, classes=classes,\n",
    "                        **kwargs)\n",
    "\n",
    "\n",
    "def EfficientNetB6(weights='imagenet',\n",
    "                   input_tensor=None,\n",
    "                   input_shape=None,\n",
    "                   pooling=None,\n",
    "                   classes=1000,\n",
    "                   **kwargs):\n",
    "    return EfficientNet(1.8, 2.6, 528, 0.5,\n",
    "                        model_name='efficientnet-b6',\n",
    "                        weights=weights,\n",
    "                        input_tensor=input_tensor, input_shape=input_shape,\n",
    "                        pooling=pooling, classes=classes,\n",
    "                        **kwargs)\n",
    "\n",
    "\n",
    "def EfficientNetB7(weights='imagenet',\n",
    "                   input_tensor=None,\n",
    "                   input_shape=None,\n",
    "                   pooling=None,\n",
    "                   classes=1000,\n",
    "                   **kwargs):\n",
    "    return EfficientNet(2.0, 3.1, 600, 0.5,\n",
    "                        model_name='efficientnet-b7',\n",
    "                        weights=weights,\n",
    "                        input_tensor=input_tensor, input_shape=input_shape,\n",
    "                        pooling=pooling, classes=classes,\n",
    "                        **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理图片\n",
    "def preprocess_input(x):\n",
    "    x /= 255.\n",
    "    x -= 0.5\n",
    "    x *= 2.\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EfficientNetB0()\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = 'image_of_predict/Koalas.jpg'\n",
    "img = image.load_img(img_path, target_size=(416, 416))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "print('Input image shape:', x.shape)\n",
    "\n",
    "preds = model.predict(x)\n",
    "print(np.argmax(preds))\n",
    "print('Predicted:', decode_predictions(preds, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
